# ChatGPT-5 and OpenAI Responses API Guide

## Overview

ChatGPT-5 is OpenAI's latest multimodal generative AI model (released August 7, 2025) that integrates text, speech, and image processing capabilities. It's a reasoning model that thinks before answering, making it excellent for complex problem-solving, coding, and multi-step planning.

**Key Features:**
- **Multimodal**: Processes text, speech, and images
- **Reasoning Model**: Uses internal chain-of-thought reasoning before responding
- **High Performance**: State-of-the-art results in mathematics, programming, finance, and multimodal understanding
- **Enhanced Safety**: Implements "safe completions" for potentially harmful queries

## The Responses API

The **Responses API** is OpenAI's recommended API for all new projects, especially when using reasoning models like GPT-5. It replaces the older Chat Completions API and provides better performance and intelligence.

### Why Use Responses API?
- Better performance with reasoning models
- More flexible input/output handling
- Improved conversation state management
- Better support for multimodal inputs

## Installation

```bash
pip install openai
```

## Basic Usage

### Simple Text Generation

```python
from openai import OpenAI

client = OpenAI()

response = client.responses.create(
    model="gpt-5",
    input="Write a one-sentence bedtime story about a unicorn."
)

print(response.output_text)
```

### With Reasoning (Recommended for GPT-5)

```python
from openai import OpenAI

client = OpenAI()

response = client.responses.create(
    model="gpt-5",
    reasoning={"effort": "medium"},  # "low", "medium", or "high"
    input="Write a bash script that takes a matrix represented as a string with format '[1,2],[3,4],[5,6]' and prints the transpose in the same format."
)

print(response.output_text)
```

## API Parameters

### Core Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `model` | string | Model identifier (e.g., "gpt-5", "gpt-5-mini", "gpt-5-nano") |
| `input` | string or array | The prompt or conversation messages |
| `reasoning` | object | Reasoning configuration (for reasoning models) |
| `instructions` | string | High-level instructions for model behavior |
| `max_output_tokens` | integer | Maximum tokens to generate (including reasoning) |

### Reasoning Parameter

```python
reasoning = {
    "effort": "medium"  # "low", "medium", or "high"
}
```

- **low**: Fastest, economical token usage
- **medium**: Balanced speed and reasoning (default)
- **high**: More complete reasoning, slower response

### Input Formats

#### Simple String Input
```python
response = client.responses.create(
    model="gpt-5",
    input="Your prompt here"
)
```

#### Message Array Input
```python
response = client.responses.create(
    model="gpt-5",
    input=[
        {
            "role": "developer",
            "content": "You are a helpful assistant."
        },
        {
            "role": "user",
            "content": "What is the capital of France?"
        }
    ]
)
```

#### With Instructions Parameter
```python
response = client.responses.create(
    model="gpt-5",
    reasoning={"effort": "low"},
    instructions="Talk like a pirate.",
    input="Are semicolons optional in JavaScript?"
)
```

## Message Roles

OpenAI models prioritize messages differently based on role:

| Role | Priority | Description |
|------|----------|-------------|
| `developer` | Highest | Instructions from application developer |
| `user` | Medium | Instructions from end user |
| `assistant` | N/A | Messages generated by the model |

Think of roles like function definitions:
- `developer` = function definition (rules and business logic)
- `user` = function arguments (inputs and configuration)

## Multimodal Inputs

### Image Analysis

```python
response = client.responses.create(
    model="gpt-5",
    input=[
        {
            "role": "user",
            "content": [
                {
                    "type": "input_text",
                    "text": "What is in this image?"
                },
                {
                    "type": "input_image",
                    "image_url": "https://example.com/image.png"
                }
            ]
        }
    ]
)

print(response.output_text)
```

### File Inputs

```python
# Upload a file first
file = client.files.create(
    file=open("document.pdf", "rb"),
    purpose="user_data"
)

# Use in response
response = client.responses.create(
    model="gpt-5",
    input=[
        {
            "role": "user",
            "content": [
                {
                    "type": "input_text",
                    "text": "Summarize this document"
                },
                {
                    "type": "input_file",
                    "file_id": file.id
                }
            ]
        }
    ]
)
```

## Understanding the Response

### Response Structure

The response contains an `output` array that can have multiple items:

```json
{
    "output": [
        {
            "id": "msg_67b73f697ba4819183a15cc17d011509",
            "type": "message",
            "role": "assistant",
            "content": [
                {
                    "type": "output_text",
                    "text": "The generated text here...",
                    "annotations": []
                }
            ]
        }
    ]
}
```

**Important**: The `output` array often has more than one item! It can contain:
- Text messages
- Tool calls
- Reasoning tokens metadata
- Other items

### Accessing Output Text

**Recommended**: Use the `output_text` property (available in SDKs) which aggregates all text outputs:

```python
print(response.output_text)  # Convenient shortcut
```

**Manual**: Access the output array directly (not recommended for simple cases):

```python
# Not recommended - output array structure may vary
for item in response.output:
    if item.type == "message":
        for content in item.content:
            if content.type == "output_text":
                print(content.text)
```

### Usage Information

```python
usage = response.usage
print(f"Input tokens: {usage.input_tokens}")
print(f"Output tokens: {usage.output_tokens}")
print(f"Total tokens: {usage.total_tokens}")

# For reasoning models:
if hasattr(usage, 'output_tokens_details'):
    reasoning_tokens = usage.output_tokens_details.reasoning_tokens
    print(f"Reasoning tokens: {reasoning_tokens}")
```

## Reasoning Models

### How Reasoning Works

1. **Reasoning Tokens**: Model generates internal "thinking" tokens (not visible)
2. **Completion Tokens**: Model produces visible answer tokens
3. **Context Management**: Reasoning tokens are discarded after response

### Managing Context Window

- Reasoning tokens count toward context window and billing
- Reserve at least 25,000 tokens for reasoning + outputs
- Use `max_output_tokens` to control costs

### Handling Incomplete Responses

```python
response = client.responses.create(
    model="gpt-5",
    reasoning={"effort": "medium"},
    input="Your prompt",
    max_output_tokens=300
)

if response.status == "incomplete":
    if response.incomplete_details.reason == "max_output_tokens":
        print("Ran out of tokens")
        if response.output_text:
            print("Partial output:", response.output_text)
        else:
            print("Ran out during reasoning")
```

## Reusable Prompts

Create prompts in the OpenAI dashboard and reference them:

```python
response = client.responses.create(
    model="gpt-5",
    prompt={
        "id": "pmpt_abc123",
        "version": "2",
        "variables": {
            "customer_name": "Jane Doe",
            "product": "40oz juice box"
        }
    }
)
```

## Tools and Function Calling

### Web Search

```python
response = client.responses.create(
    model="gpt-5",
    tools=[{"type": "web_search"}],
    input="What was a positive news story from today?"
)

print(response.output_text)
```

### Custom Functions

```python
def get_weather(location: str) -> str:
    # Your function implementation
    return f"Weather in {location}: Sunny, 72Â°F"

response = client.responses.create(
    model="gpt-5",
    tools=[
        {
            "type": "function",
            "function": {
                "name": "get_weather",
                "description": "Get weather for a location",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "City name"
                        }
                    },
                    "required": ["location"]
                }
            }
        }
    ],
    input="What's the weather in San Francisco?"
)
```

## Streaming

```python
stream = await client.responses.create(
    model="gpt-5",
    input="Your prompt here",
    stream=True
)

for event in stream:
    print(event)
```

## Conversation State Management

Use `previous_response_id` to maintain conversation context:

```python
# First message
response1 = client.responses.create(
    model="gpt-5",
    input="What is the capital of France?"
)

# Follow-up message
response2 = client.responses.create(
    model="gpt-5",
    previous_response_id=response1.id,
    input="What is its population?"
)
```

## Best Practices

### 1. Use Specific Model Snapshots in Production

```python
# Pin to specific snapshot for consistency
model = "gpt-5-2025-08-07"  # Instead of just "gpt-5"
```

### 2. Build Evals

Create evaluations to measure prompt performance as you iterate.

### 3. Use Reasoning for Complex Tasks

```python
# For complex problem-solving:
reasoning = {"effort": "high"}

# For simple queries:
reasoning = {"effort": "low"}
```

### 4. Reserve Token Budget

- Start with 25,000 token buffer for reasoning + outputs
- Adjust based on your specific use case

### 5. Migrate from Chat Completions

If you're using the Chat Completions API, migrate to Responses API for better performance with reasoning models.

## Model Variants

| Model | Description | Use Case |
|-------|-------------|----------|
| `gpt-5` | Full model | Complex tasks, broad domains |
| `gpt-5-mini` | Smaller, faster | Faster responses, lower cost |
| `gpt-5-nano` | Smallest, fastest | Cost-sensitive applications |

## Error Handling

```python
from openai import OpenAI, APIError

client = OpenAI()

try:
    response = client.responses.create(
        model="gpt-5",
        input="Your prompt"
    )
except APIError as e:
    print(f"API Error: {e.message}")
    print(f"Status: {e.status_code}")
except Exception as e:
    print(f"Error: {e}")
```

## Integration Example for Real Estate Reports

```python
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def generate_report(transcription_json: dict) -> str:
    """
    Generate real estate visit report from diarized transcription.
    
    Args:
        transcription_json: Transcription data with utterances
        
    Returns:
        Markdown report
    """
    # Prompt is now stored in OpenAI platform
    prompt_id = os.getenv("OPENAI_PROMPT_ID")
    prompt_version = os.getenv("OPENAI_PROMPT_VERSION", "1")
    
    # Format input
    input_text = f"""{prompt_template}

TRANSCRIPTION DATA:
{json.dumps(transcription_json, indent=2)}
"""
    
    # Generate report
    response = client.responses.create(
        model="gpt-5",
        reasoning={"effort": "medium"},
        instructions="You are an expert commercial analyst. Generate a professional real estate visit report.",
        input=input_text,
        max_output_tokens=8000  # Reserve space for reasoning + output
    )
    
    return response.output_text
```

## References

- [OpenAI Quickstart](https://platform.openai.com/docs/quickstart)
- [Responses API Reference](https://platform.openai.com/docs/api-reference/responses)
- [Reasoning Models Guide](https://platform.openai.com/docs/guides/reasoning)
- [Models Documentation](https://platform.openai.com/docs/models)


